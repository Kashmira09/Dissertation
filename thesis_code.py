# -*- coding: utf-8 -*-
"""Thesis Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12tI06ODS6Y_I91YhXTcYvcj8S5Om5ZTU
"""

import pandas as pd
from pandas import read_csv
dataset= read_csv('/content/drive/MyDrive/Combined Data.csv',index_col=0)
pd.set_option('display.max_colwidth', None) # Setting this so we can see the full content of cells
pd.set_option('display.max_columns', None) # to make sure we can see all the columns in output window
# dataset.shape

dataset.shape

dataset.info()

dataset.isna().sum()

dataset.dropna(inplace = True)
dataset.isna().sum()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
# Count the occurrences of each category
status_counts = dataset['status'].value_counts()

# Define colors for each category (7 colors)
colors = ['#419D78', '#E0A458', '#2D3047', '#FFDBB5', '#C04ABC', '#B3CDE0', '#D0D0D0']

# Create the pie chart
plt.figure(figsize=(7, 7))
plt.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%',
        startangle=140, colors=colors, shadow=True)

plt.title('Distribution of Mental Health Conditions')
plt.axis('equal')  # Equal aspect ratio ensures that the pie is drawn as a circle.

# Display the chart
plt.tight_layout()
plt.show()

status_counts = dataset['status'].value_counts()

plt.figure(figsize=(10, 6))
#Define colors for each category (7 colors)
colors = ['#419D78', '#E0A458', '#2D3047', '#FFDBB5', '#C04ABC', '#B3CDE0', '#D0D0']
sns.barplot(x=status_counts.index, y=status_counts.values)
plt.xlabel('Status')
plt.ylabel('Count')
plt.title('Distribution of Status Categories')
plt.xticks(rotation=45)
plt.show()

dataset['status']=dataset['status'].map({'Depression':1,'Suicidal':1,'Anxiety':1,'Stress':1,'Bipolar':1,'Personality disorder':1,'Normal':0})
dataset.info()

dataset.status.value_counts()

import re, nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download('punkt_tab')
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
import joblib

pip install lxml

# Cleaning Tweets
def cleaner(tweet):
    soup = BeautifulSoup(tweet, 'lxml') # removing HTML entities such as ‘&amp’,’&quot’,'&gt'; lxml is the html parser and shoulp be installed using 'pip install lxml'
    souped = soup.get_text()
    re1 = re.sub(r"(@|http://|https://|www|\\x)\S*", " ", souped) # substituting @mentions, urls, etc with whitespace
    re2 = re.sub("[^A-Za-z]+"," ", re1) # substituting any non-alphabetic character that repeats one or more times with whitespace

    """
    For more info on regular expressions visit -
    https://docs.python.org/3/howto/regex.html
    """

    tokens = nltk.word_tokenize(re2)
    lower_case = [t.lower() for t in tokens]

    stop_words = set(stopwords.words('english'))
    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))

    wordnet_lemmatizer = WordNetLemmatizer()
    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]
    return lemmas

dataset['cleaned_tweet'] = dataset.statement.apply(cleaner)
dataset = dataset[dataset['cleaned_tweet'].map(len) > 0] # removing rows with cleaned tweets of length 0
print("Printing top 5 rows of dataframe showing original and cleaned tweets....")
print(dataset[['statement','cleaned_tweet']].head())
dataset.drop(['statement'], axis=1, inplace=True)
# Saving cleaned tweets to csv
dataset.to_csv('cleaned_data.csv', index=False)
dataset['cleaned_tweet'] = [" ".join(row) for row in dataset['cleaned_tweet'].values] # joining tokens to create strings. TfidatasetVectorizer does not accept tokens as input
data = dataset['cleaned_tweet']
dataset.reset_index(drop=True, inplace=True)
Y = dataset['status'] # target column
tfidf = TfidfVectorizer(max_features=5000, min_df=2, max_df=0.6, ngram_range=(1,2)) # min_df=.00015 means that each ngram (unigram, bigram, & trigram) must be present in at least 30 documents for it to be considered as a token (200000*.00015=30). This is a clever way of feature engineering
tfidf.fit(data) # learn vocabulary of entire data
data_tfidf = tfidf.transform(data) # creating tfidataset values
pd.DataFrame(pd.Series(tfidf.get_feature_names_out())).to_csv('vocabulary.csv', header=False, index=False)
print("Shape of tfidf matrix: ", data_tfidf.shape)

# Implementing Support Vector Classifier
svc_clf = LinearSVC() # kernel = 'linear' and C = 1

# Running cross-validation
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
import numpy as np

kf = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)  # 5-fold cross-validation
precisions = []  # For precision
recalls = []  # For recall
iteration = 0
smote = SMOTE(random_state = 101)
for train_index, test_index in kf.split(data_tfidf, Y):
    iteration += 1
    print("Iteration ", iteration)

    # Splitting data
    X_train, Y_train = data_tfidf[train_index], Y[train_index]
    X_test, Y_test = data_tfidf[test_index], Y[test_index]

    # Fitting the model
    svc_clf.fit(X_train, Y_train)

    # Making predictions
    Y_pred = svc_clf.predict(X_test)

    # Calculating metrics
    precision = metrics.precision_score(Y_test, Y_pred, average='weighted')
    recall = metrics.recall_score(Y_test, Y_pred, average='weighted')

    # Printing metrics for each iteration
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")

    # Appending metrics
    precisions.append(precision)
    recalls.append(recall)

    # Calculating and displaying confusion matrix
    cm = confusion_matrix(Y_test, Y_pred)
    print("Confusion Matrix:\n", cm)

    # Visualizing confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svc_clf.classes_)
    disp.plot(cmap='BuPu')
    plt.title(f"Confusion Matrix - Iteration {iteration}")
    plt.show()

# Calculating mean metrics
svc_mean_precision = np.mean(precisions)
svc_mean_recall = np.mean(recalls)

# Printing mean metrics
print("Mean cross-validation precision: ", svc_mean_precision)
print("Mean cross-validation recall: ", svc_mean_recall)

from sklearn.metrics import precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay
# Initialize Naive Bayes Classifier
nbc_clf = MultinomialNB(alpha=0.3)  # Tuned alpha value after experimentation

# Running cross-validation
kf = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)  # 2-fold cross-validation
precisions = []  # For precision
recalls = []  # For recall
iteration = 0
smote = SMOTE(random_state=101)  # SMOTE for oversampling minority class

for train_index, test_index in kf.split(data_tfidf, Y):
    iteration += 1
    print("Iteration ", iteration)

    # Splitting data into training and testing sets
    X_train, Y_train = data_tfidf[train_index], Y[train_index]
    X_test, Y_test = data_tfidf[test_index], Y[test_index]

    # Applying SMOTE on the training set
    X_train_resampled, Y_train_resampled = smote.fit_resample(X_train, Y_train)
    print(f"Training data size before SMOTE: {X_train.shape}, after SMOTE: {X_train_resampled.shape}")

    # Fitting the Naive Bayes Classifier on resampled data
    nbc_clf.fit(X_train_resampled, Y_train_resampled)

    # Making predictions on the test set
    Y_pred = nbc_clf.predict(X_test)

    # Calculating metrics
    precision = precision_score(Y_test, Y_pred, average='weighted')
    recall = recall_score(Y_test, Y_pred, average='weighted')

    # Printing metrics for each iteration
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")

    # Appending metrics to calculate mean later
    precisions.append(precision)
    recalls.append(recall)

    # Calculating and displaying confusion matrix
    cm = confusion_matrix(Y_test, Y_pred)
    print("Confusion Matrix:\n", cm)

    # Visualizing the confusion matrix
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nbc_clf.classes_)
    disp.plot(cmap='BuPu')  # Lighter purple palette
    plt.title(f"Confusion Matrix - Iteration {iteration}")
    plt.show()

# Calculating mean metrics across all folds
nbc_mean_precision = np.mean(precisions)
nbc_mean_recall = np.mean(recalls)

# Printing mean metrics
print("Mean cross-validation precision: ", nbc_mean_precision)
print("Mean cross-validation recall: ", nbc_mean_recall)

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import precision_score, recall_score
from sklearn.linear_model import LogisticRegression
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Assuming the dataset has 'cleaned_tweet' (features) and 'status' (labels) columns
X = dataset['cleaned_tweet']  # Use cleaned tweets for features
Y = dataset['status']  # Target sentiment column

# Convert text data to numerical data using TF-IDF
tfidf_vectorizer = TfidfVectorizer(min_df=0.001, max_features=4000, ngram_range=(1, 2))
X_tfidf = tfidf_vectorizer.fit_transform(X)

# Split data into train and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X_tfidf, Y, test_size=0.2, random_state=1)

# Define the pipeline with SMOTE and Logistic Regression
pipeline = Pipeline([
    ('balancing', SMOTE(random_state=101)),
    ('classification', LogisticRegression(max_iter=500, random_state=1))
])

# Fit the model
pipeline.fit(X_train, Y_train)

# Evaluate the model on test data
Y_pred = pipeline.predict(X_test)

# Calculate Precision and Recall
precision = precision_score(Y_test, Y_pred, average='weighted')
recall = recall_score(Y_test, Y_pred, average='weighted')

# Print Precision and Recall
print(f"\nPrecision (Weighted): {precision:.4f}")
print(f"Recall (Weighted): {recall:.4f}")